#!/bin/bash
# build.sh - Master Build Dispatcher for [MODULE_NAME] (v2.4.0 Template)
# Generated by LLM Cross-Compiler Framework (Ditto/Wizard)
#
# Environment Variables (Injected by Orchestrator):
# $MODEL_SOURCE    - Path to input model
# $QUANTIZATION    - Target Quantization (e.g. Q4_K_M, INT8)
# $OUTPUT_DIR      - Destination for artifacts
# $BUILD_JOBS      - Number of parallel jobs
# $MODEL_TASK      - Task Type (LLM, VOICE, VLM)
# $JOB_TYPE        - 'build' (default) or 'imatrix'
# $USE_IMATRIX     - '1' or '0' (for build job)
# $IMATRIX_PATH    - Path to pre-calculated matrix

set -euo pipefail

# --- CONFIGURATION & PATHS ---
WORK_DIR="${BUILD_CACHE_DIR:-/build-cache}"
OUTPUT_DIR="${WORK_DIR}/output"
IMATRIX_DIR="${WORK_DIR}/imatrix"
mkdir -p "$OUTPUT_DIR"
mkdir -p "$IMATRIX_DIR"

JOB_TYPE="${JOB_TYPE:-build}"
QUANT_TYPE="${QUANTIZATION:-FP16}"

echo "=== Build Started: [MODULE_NAME] ==="
echo "Model: $MODEL_SOURCE"
echo "Task:  ${MODEL_TASK:-LLM}"
echo "Quant: $QUANT_TYPE"
echo "Job:   $JOB_TYPE"

# --- TOOL DETECTION (Generic) ---
# Try to locate llama.cpp tools if available (Standard for many embedded flows)
LLAMA_BASE="/usr/src/llama.cpp"
CONVERT_SCRIPT=""
IMATRIX_BIN=""
QUANTIZE_BIN=""

if [ -f "$LLAMA_BASE/convert-hf-to-gguf.py" ]; then CONVERT_SCRIPT="$LLAMA_BASE/convert-hf-to-gguf.py"; fi
if [ -x "$LLAMA_BASE/llama-imatrix" ]; then IMATRIX_BIN="$LLAMA_BASE/llama-imatrix"; fi
if [ -x "$LLAMA_BASE/llama-quantize" ]; then QUANTIZE_BIN="$LLAMA_BASE/llama-quantize"; fi

# Search in PATH if not found
if [ -z "$IMATRIX_BIN" ] && command -v llama-imatrix &> /dev/null; then IMATRIX_BIN=$(command -v llama-imatrix); fi
if [ -z "$QUANTIZE_BIN" ] && command -v llama-quantize &> /dev/null; then QUANTIZE_BIN=$(command -v llama-quantize); fi

# ==============================================================================
# 0. SPECIAL JOB: IMATRIX GENERATION
# ==============================================================================
if [[ "$JOB_TYPE" == "imatrix" ]]; then
    echo ">> [IMatrix] Starting Importance Matrix Calculation..."
    
    DATASET="${DATASET_PATH:-}"
    if [ ! -f "$DATASET" ]; then
        echo "❌ Error: Dataset file not found at '$DATASET'"
        exit 1
    fi

    if [ -z "$IMATRIX_BIN" ] || [ -z "$CONVERT_SCRIPT" ]; then
        echo "❌ Error: llama-imatrix or conversion script not found in this container."
        echo "   This container might not support GGUF/IMatrix workflows."
        exit 1
    fi

    # 1. Convert to F16 (Intermediate)
    echo ">> [IMatrix] Converting to intermediate F16 GGUF..."
    INTERMEDIATE="/tmp/model-f16.gguf"
    # We use python3 explicitly
    python3 "$CONVERT_SCRIPT" "$MODEL_SOURCE" --outfile "$INTERMEDIATE" --outtype f16

    # 2. Calculate Matrix
    echo ">> [IMatrix] Calculating matrix (chunks: 100)..."
    OUTPUT_DAT="$IMATRIX_DIR/imatrix.dat"
    
    "$IMATRIX_BIN" -m "$INTERMEDIATE" -f "$DATASET" -o "$OUTPUT_DAT" --chunks 100
    
    echo ">> [IMatrix] Success. Matrix saved to $OUTPUT_DAT"
    rm -f "$INTERMEDIATE"
    exit 0
fi

# ==============================================================================
# 1. NORMAL BUILD PREPARATION
# ==============================================================================

# Prepare IMatrix Args for the Logic Block
IMATRIX_ARGS=""
if [[ "${USE_IMATRIX:-0}" == "1" ]]; then
    MATRIX_FILE="${IMATRIX_PATH:-$IMATRIX_DIR/imatrix.dat}"
    if [ -f "$MATRIX_FILE" ]; then
        echo ">> [Build] Using IMatrix: $MATRIX_FILE"
        IMATRIX_ARGS="--imatrix $MATRIX_FILE"
    else
        echo "⚠️ Warning: IMatrix requested but file '$MATRIX_FILE' not found. Proceeding without."
    fi
fi

# --- SDK SETUP ---
# [SDK_SETUP_COMMANDS]

# --- CONVERSION & QUANTIZATION LOGIC ---
echo ">> Starting Logic for Task: ${MODEL_TASK:-LLM} / Quant: $QUANT_TYPE"

case "$QUANT_TYPE" in
[QUANTIZATION_LOGIC]

    *)
        echo ">> No specific quantization logic matched for '$QUANT_TYPE'."
        echo ">> Fallback Strategy: Passthrough (Copy Original Model)."
        
        if [ -d "$MODEL_SOURCE" ]; then
            # Hugging Face Repo (Directory) -> Copy content
            cp -r "$MODEL_SOURCE/"* "$OUTPUT_DIR/"
        elif [ -f "$MODEL_SOURCE" ]; then
            # Single File (GGUF/ONNX) -> Copy file
            cp "$MODEL_SOURCE" "$OUTPUT_DIR/"
        else
            echo "❌ Error: Model source '$MODEL_SOURCE' not found or invalid type."
            exit 1
        fi
        
        echo ">> Original model copied to output (FP16/Source Precision)."
        ;;
esac

# --- PACKAGING ---
echo "=== Packaging Artifacts ==="
# [PACKAGING_COMMANDS]

echo "Build completed successfully."
