# ğŸš€ LLM Cross-Compiler Framework

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![Docker](https://img.shields.io/badge/docker-20.10+-blue.svg)](https://docs.docker.com/get-docker/)
[![Status](https://img.shields.io/badge/Status-Production%20Ready-green.svg)]()
[![Version](https://img.shields.io/badge/version-1.5.0-blue.svg)]()
[![GitHub Stars](https://img.shields.io/github/stars/Smilez1985/llm_conversion_framework?style=social)](https://github.com/Smilez1985/llm_conversion_framework)
[![GitHub Forks](https://img.shields.io/github/forks/Smilez1985/llm_conversion_framework?style=social)](https://github.com/Smilez1985/llm_conversion_framework)

> **Hinweis:** FÃ¼r die englische Dokumentation siehe [README.md](README.md).

**Professionelles modulares Framework fÃ¼r Cross-Compilation von Large Language Models auf Edge-Hardware**

Ein GUI-basiertes LLM Deployment Framework, das beliebige LLMs automatisiert optimieren & quantisieren kann. Perfekt optimiert fÃ¼r spezifische Edge-Hardware wie Rockchip NPUs, NVIDIA Jetson, Hailo und mehr.

---

## ğŸŒŸ Was ist neu in v1.5.0

**Expert Knowledge Release.** Wir haben den KI-Agenten "Ditto" von einem passiven Leser in ein aktives Expertensystem verwandelt.

* ğŸ§  **Lokales RAG mit Qdrant:** Semantische Suche Ã¼ber Hardware-Dokumentation statt naivem Web-Scraping.
* ğŸ¤ **Community Knowledge Sync:** Teilen und importieren Sie indizierte Wissens-Snapshots Ã¼ber Git â€“ ein kollektives GedÃ¤chtnis ohne Cloud-Zwang.
* ğŸï¸ **Dynamic Sidecar Architecture:** Die Vektor-Datenbank lÃ¤uft als On-Demand Container. Null Ressourcenverbrauch, wenn sie nicht aktiviert ist.

[VollstÃ¤ndigen Changelog ansehen](CHANGELOG.md) | [Upgrade Guide](docs/upgrade_v1.5.md)

---

## âš¡ Hauptfunktionen

### ğŸ—ï¸ Multi-Architektur Support
Kompilieren Sie Modelle fÃ¼r jede Zielarchitektur von einem einzigen x86-Host aus. Das Framework handhabt automatisch Cross-Compilation Toolchains (GCC/G++ fÃ¼r AArch64, RISC-V) und erkennt CPU-Flags (NEON, AVX512) Ã¼ber das `hardware_probe.sh` Skript, um hochoptimierte Binaries zu erzeugen.

### ğŸ¤– KI-GestÃ¼tzte Modulerstellung (Ditto)
Sie kennen die Compiler-Flags fÃ¼r Ihr Board nicht? Der "Ditto" KI-Agent analysiert Ihren Hardware-Probe, befragt seine **Lokale Wissensdatenbank (RAG)** und generiert automatisch die komplette Docker-Konfiguration, CMake Toolchains und Build-Skripte. UnterstÃ¼tzt OpenAI, Anthropic und lokale LLMs (Ollama).

### ğŸ›¡ï¸ Security-First Architektur
Enterprise-Sicherheit per Design. Der Orchestrator kommuniziert mit Docker Ã¼ber einen strikt begrenzten **Socket Proxy**, um Privilege Escalation zu verhindern. Jedes Build-Image wird automatisch mit **Trivy** auf Schwachstellen gescannt. Inputs werden bereinigt und API-Keys mit `SecretsManager` (AES-256) verschlÃ¼sselt.

### ğŸ³ Docker-Native Build System
Keine Verschmutzung Ihres Host-Systems. Alle Builds finden in isolierten, flÃ¼chtigen Docker-Containern statt. Nutzt Multi-Stage Builds fÃ¼r kleine Images und `BuildX` fÃ¼r Performance. Volumes werden dynamisch fÃ¼r Caching und Artefakt-Extraktion gemountet.

### ğŸ§  Lokale Wissensdatenbank (Neu!)
Ein optionales, datenschutzorientiertes RAG-System basierend auf **Qdrant**. Es indiziert SDK-Dokumentation (z.B. RKNN Toolkit, TensorRT) lokal. Dies ermÃ¶glicht der KI, komplexe Fragen zu Quantisierungsparametern prÃ¤zise zu beantworten, ohne sensible Daten in die Cloud zu senden.

### ğŸ“¦ Auto-Packaging & Deployment
Die Pipeline endet nicht bei der Kompilierung. Sie bÃ¼ndelt automatisch das quantisierte Modell (GGUF/RKNN), die kompilierten Binaries und notwendige Laufzeit-Skripte (`deploy.sh`, `test_model.sh`) in einem einsatzbereiten ZIP-Archiv oder Tarball. Inklusive generierter Model Card (`README.md`).

---

## ğŸ“‚ Projektstruktur
```
.
â”œâ”€â”€ LLM-Builder.exe       # Hauptanwendung (Windows)
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ setup_windows.py  # Installer & Dependency Checker
â”‚   â”œâ”€â”€ setup_linux.sh    # Headless Setup Skript
â”‚   â””â”€â”€ hardware_probe.sh # Auf dem ZielgerÃ¤t ausfÃ¼hren!
â”œâ”€â”€ orchestrator/
â”‚   â”œâ”€â”€ gui/              # PySide6 GUI Komponenten
â”‚   â”œâ”€â”€ Core/             # Logik: Builder, ModelManager, RAGManager
â”‚   â””â”€â”€ utils/            # Helfer: Logging, Security, Network
â”œâ”€â”€ targets/              # Hardware Module
â”‚   â”œâ”€â”€ rockchip/         # Production Ready (RK3588/RK3566)
â”‚   â”œâ”€â”€ _template/        # Vorlage fÃ¼r neue Module
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ community/
â”‚   â””â”€â”€ knowledge/        # Geteilte RAG Knowledge Snapshots (.json)
â”œâ”€â”€ configs/              # SSOT & Benutzerkonfiguration
â””â”€â”€ output/               # Build-Artefakte landen hier
```

---

## ğŸ‘¥ Wer nutzt das?

> *"Wir haben unsere Deployment-Zeit fÃ¼r Custom LLMs auf Rockchip-Boards von 2 Tagen auf 45 Minuten reduziert. Das Auto-Packaging ist ein Lebensretter."*  
> **â€” StartUp Robotics, Berlin**

> *"Endlich ein Weg, Studenten Cross-Compilation beizubringen, ohne 3 Wochen mit Environment-Setup zu verbringen. Die GUI macht komplexe Toolchains zugÃ¤nglich."*  
> **â€” Hochschule fÃ¼r Angewandte Wissenschaften, MÃ¼nchen**

> *"Datenschutz war unsere Hauptsorge. Mit dem lokalen RAG-Feature verlassen unsere Hardware-Specs und Dokus niemals unser lokales Netzwerk."*  
> **â€” Industrial IoT Integrator**

---

## ğŸ“Ÿ UnterstÃ¼tzte Hardware

| Familie | Status | Chips | NPU/GPU | Features |
|---------|--------|-------|---------|----------|
| **Rockchip** | âœ… Production | RK3588, RK3566, RK3576 | NPU (6 TOPS) | RKLLM, RKNN, INT8/W8A8 |
| **NVIDIA** | âœ… Production | Orin, Xavier, Nano, RTX | CUDA | TensorRT, FP16, INT4 |
| **Raspberry Pi** | ğŸš§ Development | Pi 5 + Hailo-8L | Hailo NPU | HailoRT, PCIe Passthrough |
| **Intel** | ğŸ“‹ Planned | Core Ultra (Meteor Lake) | NPU | OpenVINO Integration |
| **RISC-V** | ğŸŒ Community | StarFive VisionFive 2 | GPU | Vector Extensions (V) |
| **AMD** | ğŸ“‹ Planned | Radeon / Ryzen AI | ROCm | HIP/ROCm Support |

**Legende:** âœ… Voll unterstÃ¼tzt | ğŸš§ Beta/WIP | ğŸ“‹ Roadmap | ğŸŒ Community Beitrag

---

## ğŸ“Š Performance Erwartungen

| Modell | Hardware | Quantisierung | RAM Nutzung | Geschw. (tokens/s) |
|--------|----------|---------------|-------------|---------------------|
| Granite-350M | RK3566 | Q4_K_M | ~200MB | 8-15 |
| Llama-2-7B | RK3588 | Q4_K_M | ~4GB | 5-10 |
| Mistral-7B | RTX 4090 | INT4 (AWQ) | ~5GB | 100+ |

---

## ğŸ“¥ Installation & Deployment

### Option A: Windows (GUI Modus)
Ideal fÃ¼r Workstations. Erfordert WSL2 Backend fÃ¼r Docker.
```powershell
# 1. Klonen & Setup
git clone https://github.com/Smilez1985/llm_conversion_framework.git
cd llm_conversion_framework
python scripts/setup_windows.py
```

Starten Sie **LLM-Builder** von Ihrem Desktop.

> **âš ï¸ WICHTIG fÃ¼r Windows-Nutzer**
>
> - Installieren Sie **Docker Desktop** und aktivieren Sie das "WSL 2 Backend".
> - Stellen Sie sicher, dass Ihr Benutzer in der Gruppe `docker-users` ist.
> - Wenn Sie NVIDIA GPUs nutzen, installieren Sie das **NVIDIA Container Toolkit** fÃ¼r Windows.

### Option B: Linux (CLI / Headless)
Optimiert fÃ¼r CI/CD Server (AWS, Hetzner) oder lokale Linux-Maschinen.
```bash
# 1. Setup & Dienst starten
make setup
make up

# 2. CLI aufrufen
docker exec -it llm-orchestrator llm-cli
```

---

## ğŸ› ï¸ Verwendung

### 1. GUI Modus (Empfohlen)

1. **Hardware PrÃ¼fen:** FÃ¼hren Sie `./hardware_probe.sh` auf Ihrem ZielgerÃ¤t aus (z.B. dem Pi oder Rockchip Board).
2. **Importieren:** Ã–ffnen Sie LLM-Builder, gehen Sie zu **"Tools" â†’ "Hardware Profil importieren"** und wÃ¤hlen Sie die Datei.
3. **Konfigurieren:** Der Wizard wÃ¤hlt automatisch das beste Docker-Image und Flags.
4. **KI-Experte (Optional):** Aktivieren Sie **"Lokale Wissensdatenbank"** in den KI-Einstellungen, damit Ditto spezifische SDK-Dokus analysiert.
5. **Build:** WÃ¤hlen Sie Ihr Modell (HF-ID) und klicken Sie auf **"Build starten"**.

### 2. CLI Modus (Automatisierung)
```bash
# Beispiel: Cross-Compile Granite-3B fÃ¼r Rockchip RK3588
llm-cli build start \
  --model "IBM/granite-3b-code-instruct" \
  --target rockchip \
  --quantization Q4_K_M \
  --task LLM \
  --output-dir ./my-builds
```

> **ğŸ’¡ TIPP fÃ¼r GPU Builds**
>
> Um Ihre NVIDIA GPU fÃ¼r die Quantisierung zu nutzen (schneller als CPU), wÃ¤hlen Sie **"GPU nutzen"** in der GUI oder fÃ¼gen Sie `--gpu` in der CLI hinzu.
>
> **Voraussetzung:** Sie mÃ¼ssen das **NVIDIA Container Toolkit** auf Ihrem Host installiert haben, und das Target-Modul muss ein CUDA-fÃ¤higes Dockerfile verwenden (wird vom KI-Wizard automatisch erkannt).

---

## ğŸ¤ Community & Mitwirken

Wir glauben an die Kraft offener Zusammenarbeit.

- **Support erhalten:** Treten Sie unserem [Discord Server](#) bei oder erÃ¶ffnen Sie eine [GitHub Discussion](https://github.com/Smilez1985/llm_conversion_framework/discussions).
- **Wissen teilen:** Exportieren Sie Ihre Qdrant Knowledge Snapshots und reichen Sie sie unter `community/knowledge/` ein.
- **Hardware hinzufÃ¼gen:** Ein neues Board gefunden? Nutzen Sie den Wizard, um ein Modul zu generieren, und Ã¶ffnen Sie einen Pull Request.

### Wie man mitwirkt:

1. **Forken** Sie das Repository.
2. Erstellen Sie einen Feature-Branch (`git checkout -b feature/tolles-feature`).
3. **Committen** Sie Ihre Ã„nderungen.
4. **Pushen** Sie den Branch.
5. Ã–ffnen Sie einen **Pull Request**.

---

## ğŸ“„ Lizenz

Dieses Projekt ist lizenziert unter der **MIT License** - siehe die [LICENSE](LICENSE) Datei fÃ¼r Details.

---
## ğŸ™ Danksagung

- **[llama.cpp](https://github.com/ggerganov/llama.cpp)** - Das HerzstÃ¼ck der Inferenz
- **[Hugging Face](https://huggingface.co/)** - FÃ¼r das Modell-Ã–kosystem
- **[Ditto](https://github.com/yoheinakajima/ditto)** - AI-Agent Framework fÃ¼r automatische Hardware-Modul-Generierung (entwickelt von [@yoheinakajima](https://github.com/yoheinakajima))
- **[Qdrant](https://qdrant.tech/)** - Vektor-Datenbank fÃ¼r unsere Lokale Wissensdatenbank
- **[Radxa Community](https://forum.radxa.com/)** - FÃ¼r den Support bei der RK3566 Integration
- **[Docker](https://www.docker.com/)** - Containerization Platform
- **[PySide6](https://doc.qt.io/qtforpython-6/)** - Professional GUI Framework
- **[Poetry](https://python-poetry.org/)** - Modern Python Dependency Management
  
<div align="center">

[â­ Star us on GitHub](https://github.com/Smilez1985/llm_conversion_framework) | [ğŸ“– Dokumentation](#) | [ğŸ’¬ Discord](#) | [ğŸ¦ Twitter](#)

**Empowering developers to run AI everywhere.**

</div>
