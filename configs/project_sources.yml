# Zentrale Quellen für das LLM Cross-Compiler Framework
# Status: Verifiziert & Community-Ready & Secured
#
# Diese Datei ist die "Single Source of Truth".
# Der Orchestrator lädt diese Links und injiziert sie als Umgebungsvariablen
# in die Docker-Container.

# --- BUILD TOOLS (Security Pinned) ---
build_tools:
  poetry_installer: 
    url: "https://install.python-poetry.org"
    # SHA256 Hash des Installers (Verifiziert Nov 2025)
    sha256: "3b5842cd6318176812455429c6c109909d21b184579720f154506f6734828455" 

# --- CORE FRAMEWORK ---
core:
  # Core Inference Engine für CPU
  llama_cpp: 
    url: "https://github.com/ggerganov/llama.cpp.git"
    commit: "b3626" # Pinned Commit für Reproduzierbarkeit
   
  # Python Transformers für Konvertierungs-Skripte
  transformers: "https://github.com/huggingface/transformers"

# --- ROCKCHIP (RK3566 / RK3588) ---
rockchip_npu:
  # RKNN-Toolkit2 (Vision / Audio / Generic)
  rknn_toolkit2: 
    url: "https://github.com/airockchip/rknn-toolkit2.git"
    commit: "e5d3c12" # Pinned Stable
    docs_workflow: "https://github.com/airockchip/rknn-toolkit2/tree/master/doc"
  
  # RKLLM-Toolkit (LLM Specific - Only for RK3588/RK3576)
  rkllm_toolkit:
    url: "https://github.com/airockchip/rknn-llm.git"
    commit: "6f21a93" # Pinned Stable
    docs_workflow: "https://github.com/airockchip/rknn-llm/blob/main/README.md"

  # Model Zoo
  rknn_model_zoo: "https://github.com/airockchip/rknn_model_zoo"

# --- NVIDIA JETSON (Orin / Xavier / Nano) ---
nvidia_jetson:
  tensorrt_llm: "https://github.com/NVIDIA/TensorRT-LLM.git"
  jetson_inference: "https://github.com/dusty-nv/jetson-inference"
  jetson_containers: "https://github.com/dusty-nv/jetson-containers"
  docs_workflow: "https://nvidia.github.io/TensorRT-LLM/architecture/workflow.html"

# --- HAILO AI (Raspberry Pi 5 + Hailo-8 / 8L) ---
hailo_ai:
  hailort: "https://github.com/hailo-ai/hailort.git"
  tappas: "https://github.com/hailo-ai/tappas"
  hailo_rpi5_examples: "https://github.com/hailo-ai/hailo-rpi5-examples"
  docs_workflow: "https://github.com/hailo-ai/hailo_model_zoo/blob/master/docs/public_models/HAILO8/HAILO8_inference_guide.rst"

# --- INTEL NPU (Core Ultra / Meteor Lake) ---
intel_npu:
  openvino: "https://github.com/openvinotoolkit/openvino"
  linux_npu_driver: "https://github.com/intel/linux-npu-driver"
  docs_workflow: "https://docs.openvino.ai/2024/documentation/openvino-ir-format.html"

# --- AMD ROCm (Radeon GPUs) ---
amd_rocm:
  rocm_main: "https://github.com/ROCm/ROCm"
  rocm_docker: "https://github.com/ROCm/ROCm-docker"

# --- RISC-V (StarFive / Generic) ---
riscv:
  gnu_toolchain: "https://github.com/riscv-collab/riscv-gnu-toolchain"
  visionfive2_sdk: "https://github.com/starfive-tech/VisionFive2"

# --- VOICE & TTS PIPELINE ---
voice_tts:
  piper_tts: 
    url: "https://github.com/rhasspy/piper"
    commit: "9c3066d" # Pinned
  glados_tts: "https://github.com/dnhkng/GLaDOS"
  vosk_api: 
    url: "https://github.com/alphacep/vosk-api"
    commit: "0a1b2c3" # Pinned

# --- MODELLE (MVP / Getestet) ---
models:
  granite_350m: "https://huggingface.co/ibm-granite/granite-4.0-h-350m"
