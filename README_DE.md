# ğŸš€ LLM Cross-Compiler Framework

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![Docker](https://img.shields.io/badge/docker-20.10+-blue.svg)](https://docs.docker.com/get-docker/)
[![Status](https://img.shields.io/badge/Status-Production%20Ready-green.svg)]()
[![Version](https://img.shields.io/badge/version-1.3.0-blue.svg)]()

> **Hinweis:** FÃ¼r die englische Dokumentation siehe [README.md](README.md).

**Professionelles modulares Framework fÃ¼r Cross-Compilation von Large Language Models auf Edge-Hardware**

Ein GUI-basiertes LLM Deployment Framework, das beliebige LLMs automatisiert optimieren & quantisieren kann. Perfekt optimiert fÃ¼r jede CPU, GPU oder NPU (Rockchip, NVIDIA, etc.).

---

## ğŸŒŸ Status: Production Ready (v1.3.0)

Das Framework wurde einem umfassenden Sicherheits- und Architektur-Audit unterzogen. Es erfÃ¼llt Enterprise-Standards hinsichtlich ModularitÃ¤t, Sicherheit (Trivy Scanning, Socket Proxy) und StabilitÃ¤t.

* **Sicherheit:** Container sind isoliert (Socket Proxy), Docker-Socket ist geschÃ¼tzt, Inputs werden validiert.
* **ModularitÃ¤t:** Klare Trennung zwischen Orchestrator (Management), Builder (AusfÃ¼hrung) und Target-Modulen.
* **AI-Integration:** Optionaler "Ditto"-Agent (v1.2) zur vollautomatischen Generierung neuer Hardware-Module.
* **I18n:** VollstÃ¤ndige UnterstÃ¼tzung fÃ¼r deutsche und englische OberflÃ¤chen.

---

## ğŸ—ºï¸ Roadmap

**v1.3.0** (Aktuell)
- âœ… AI Wizard (Ditto Integration) mit Auto-Discovery
- âœ… Sicherheits-HÃ¤rtung (Socket Proxy, Trivy Scanner)
- âœ… Multi-Provider AI Support (Ollama, OpenAI, Anthropic)
- âœ… NVIDIA GPU Passthrough Support
- âœ… Internationalisierung (DE/EN)

**v1.4.0** (Q2 2026)
- ğŸ¯ Intel NPU Support (OpenVINO) Vollintegration
- ğŸ¯ Hailo NPU Support Vollintegration
- ğŸ¯ Auto-Optimization Engine (Grid Search fÃ¼r Quantisierung)

**v2.0.0** (Q3 2026)
- ğŸ¯ Cloud Build Support (AWS/Azure Integration)
- ğŸ¯ Model Zoo Integration (One-Click Deploy)

---

## ğŸ“Š Performance Erwartungen

| Modell       | Hardware | Quantisierung | RAM Nutzung | Geschw. (tokens/s) |
| :---         | :---     | :---          | :---        | :---               |
| Granite-350M | RK3566   | Q4_K_M        | ~200MB      | 8-15               |
| Llama-2-7B   | RK3588   | Q4_K_M        | ~4GB        | 5-10               |
| Mistral-7B   | RTX 4090 | INT4 (AWQ)    | ~5GB        | 100+               |

---

## ğŸ“¥ Installation & Deployment

Das Framework unterstÃ¼tzt zwei primÃ¤re Betriebsmodi:

### A. Windows (Workstation / Laptop)
Ideal fÃ¼r Entwicklung, GUI-Nutzung und Tests.

* **Voraussetzungen:** Docker Desktop, WSL2.
* **Setup:**
    ```powershell
    # Startet den automatischen Installer (lÃ¤dt Dependencies, erstellt Shortcuts)
    python scripts/setup_windows.py
    ```
* Starten Sie danach einfach die erstellte Desktop-VerknÃ¼pfung `LLM-Builder`.

### B. Linux (Server / Headless / Cloud)
Optimiert fÃ¼r CI/CD-Pipelines, Build-Server (AWS, Hetzner) oder lokale Linux-Maschinen. LÃ¤uft effizient ohne GUI.

* **Voraussetzungen:** Docker Engine (`docker-ce`). **Kein** Docker Desktop erforderlich!
* **Setup & Start:**
    ```bash
    # PrÃ¼ft Voraussetzungen, installiert Docker bei Bedarf und korrigiert Rechte
    make setup
    
    # Startet den Orchestrator im Hintergrund (Headless Mode)
    make up
    ```
* Nutzen Sie danach die CLI: `docker exec -it llm-orchestrator llm-cli`

---

## âš™ï¸ Hardware-Nutzung & Performance

Das Framework verwaltet verfÃ¼gbare Ressourcen intelligent basierend auf Ihrer Target-Auswahl.

### Standard: CPU & RAM (Cross-Compilation)
FÃ¼r Targets wie **Rockchip (RK3588/RK3566)** nutzt der Standard-Container primÃ¤r **CPU und RAM**.

* **Warum?** Wir installieren explizit die PyTorch-CPU-Version, um das Docker-Image klein zu halten (~2GB statt >8GB).
* **Flaschenhals:** Bei der Quantisierung (z.B. `llama-quantize`) ist meist die Speicherbandbreite der limitierende Faktor, nicht die reine GPU-Rechenleistung. Eine starke CPU ist hier oft effizienter als der Overhead groÃŸer GPU-Container.

### Option: GPU-Beschleunigung (NVIDIA Jetson / RTX)
Der Framework-Kern ist **GPU-Ready**.

* **Der "Hidden Gem":** Der Builder (`orchestrator/Core/builder.py`) kann GPU-Ressourcen via `DeviceRequest` direkt an den Build-Container durchreichen.
* **Aktivierung:**
    1.  WÃ¤hlen Sie **"GPU nutzen"** im GUI-Wizard oder der CLI.
    2.  Stellen Sie sicher, dass das Target-Modul ein GPU-fÃ¤higes Basis-Image nutzt (z.B. `nvidia/cuda:12.2...`).
    3.  *Tipp:* Nutzen Sie den **AI-Wizard (Ditto)** â€“ er erkennt NVIDIA-Hardware im Probe-Log und schlÃ¤gt automatisch das passende CUDA-Image vor.

---

## ğŸ› ï¸ Features

* **Smart Wizard:** Erstellen Sie neue Hardware-Targets in 5 Schritten.
* **AI Auto-Discovery:** Laden Sie den `hardware_probe.sh` Output hoch, und die KI konfiguriert das Modul fÃ¼r Sie (Flags, SDKs, Docker Image).
* **Multi-Target:** UnterstÃ¼tzt Rockchip (NPU), NVIDIA (CUDA), Intel (OpenVINO) und mehr.
* **Security First:** Integrierter Trivy-Scanner prÃ¼ft jedes Image nach dem Build.

## ğŸ† Beispiele

### Rockchip RK3566 Beispiel

```bash
# 1. Hardware-Profil erstellen (auf dem Board)
./hardware_probe.sh
```
```bash
# 2. Build via CLI (auf dem Host)
llm-cli build start \
  --model "IBM/granite-3b-code-instruct" \
  --target rockchip \
  --quantization Q4_K_M \
  --task LLM
```
```bash
  # 3. Output: granite-3b_q4km_aarch64.zip
# EnthÃ¤lt: Quantisiertes Model + AArch64 Binary + Test Scripts
```



## ğŸ“š Documentation

- ğŸ“– [Getting Started Guide](docs/getting-started.md)
- ğŸ”§ [Adding New Targets](docs/adding-targets.md)
- ğŸ¤– [AI-Wizard "Ditto" Guide](docs/ai-wizard.md)
- ğŸ“¡ [API Reference](docs/api-reference.md)
- ğŸ’¡ [Examples & Tutorials](docs/examples/)

---

## ğŸ› ï¸ Development

### Testing
```bash
# Framework-Tests
poetry run pytest
```
```bash
# Target-Validation
./scripts/validate-target.sh targets/rockchip
```
```bash
# Integration-Test
poetry run llm-cli test --target rockchip --model test-model
```

### Module-Entwicklung Guidelines

**Goldstandard-Direktiven fÃ¼r alle Module:**

**Docker-Container:**
- âœ… Multi-Stage Build verwenden
- âœ… BuildX fÃ¼r Multi-Architektur
- âœ… Hadolint-konforme Syntax
- âœ… Poetry fÃ¼r Python-Dependencies

**Scripts (Shell/Python):**
- âœ… VollstÃ¤ndig funktionsfÃ¤hig (keine Platzhalter)
- âœ… Robuste `if not exist` Abfragen
- âœ… Professional dokumentiert/kommentiert
- âœ… Isolierte Umgebungen (Container-native)

---

## ğŸ“„ Lizenz

Dieses Projekt ist lizenziert unter der **MIT License** - siehe die [LICENSE](LICENSE) Datei fÃ¼r Details.

---

## ğŸ™ Danksagung

- **[llama.cpp](https://github.com/ggerganov/llama.cpp)** - Das HerzstÃ¼ck der Inferenz
- **[Hugging Face](https://huggingface.co/)** - FÃ¼r das Modell-Ã–kosystem
- **[Ditto](https://github.com/yoheinakajima/ditto)** - AI-Agent Framework fÃ¼r automatische Hardware-Modul-Generierung (entwickelt von [@yoheinakajima](https://github.com/yoheinakajima))
- **[Radxa Community](https://forum.radxa.com/)** - FÃ¼r den Support bei der RK3566 Integration
- **[Docker](https://www.docker.com/)** - Containerization Platform
- **[PySide6](https://doc.qt.io/qtforpython-6/)** - Professional GUI Framework
- **[Poetry](https://python-poetry.org/)** - Modern Python Dependency Management

---

<div align="center">

**Built with â¤ï¸ for the Edge AI Community**

*Empowering developers to run AI everywhere.*

</div>
