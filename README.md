# ğŸš€ LLM Cross-Compiler Framework

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![Docker](https://img.shields.io/badge/docker-20.10+-0db7ed.svg)](https://docs.docker.com/get-docker/)
[![Poetry](https://img.shields.io/badge/poetry-1.5+-60A5FA.svg)](https://python-poetry.org/)
[![Platform](https://img.shields.io/badge/platform-win%20%7C%20linux%20%7C%20mac-lightgrey)]()
[![Status](https://img.shields.io/badge/status-production-green)]()

**Professionelles modulares Framework fÃ¼r die Cross-Compilation von Large Language Models auf Edge-Hardware**

Eliminiert die KomplexitÃ¤t der Cross-Kompilierung und Quantisierung von LLMs fÃ¼r fragmentierte Edge-Hardware (CPUs, GPUs, NPUs). Community-driven, Docker-basiert, production-ready.

---

## ğŸ“– Ãœber das Projekt

Wir lÃ¶sen ein Problem, das jeder kennt, aber niemand angeht: Die saubere, reproduzierbare Kompilierung von LLMs und NPU-Tools fÃ¼r unterschiedliche Hardware-Architekturen.

Das **LLM Cross-Compiler Framework** ist keine einfache "Installations-Anleitung", sondern eine Docker-basierte FertigungsstraÃŸe. Es verwandelt Source-Code (HuggingFace, llama.cpp, Vosk) vollautomatisch in optimierte Binaries fÃ¼r dein Zielsystem.

### âœ¨ Was es leistet (V 1.1.0)

* âœ… VollstÃ¤ndige Cross-Compilation fÃ¼r **Rockchip RK3566/RK3588** (inkl. NPU-Support via RKNN)
* âœ… **Windows-Installer & GUI** fÃ¼r einfache Bedienung ohne Kommandozeilen-Frust
* âœ… **Single-Source-of-Truth** Architektur fÃ¼r reproduzierbare Builds
* âœ… **Auto-Update** & **Smart-Sync** Technologie fÃ¼r nahtlose Updates

---

## ğŸ¯ Features

| Feature | Beschreibung |
|---------|--------------|
| ğŸ—ï¸ **Multi-Arch Support** | ARM, x86_64, RISC-V mit automatischer Hardware-Erkennung |
| ğŸ³ **Docker-Native** | Isolierte Build-Umgebungen mit Multi-Stage Builds (Keine Dependency-HÃ¶lle auf dem Host) |
| ğŸ¨ **Profi-GUI** | PySide6 Interface mit integriertem **5-Schritt Module Creation Wizard** |
| âš¡ **Live Monitoring** | Echtzeit-Anzeige von Build-Logs und Fortschritt |
| ğŸ”§ **Hardware-Optimiert** | Setzt automatisch CPU-spezifische Flags (NEON, AVX, NPU) fÃ¼r maximale Performance |
| ğŸŒ **Community Hub** | Integrierter "App Store" zum Herunterladen neuer Hardware-Targets |
| ğŸ“¦ **Auto-Packaging** | Erstellt fertige Deployment-Pakete inkl. Test-Skripten fÃ¼r das ZielgerÃ¤t |

---

## ğŸš€ Quick Start

### Voraussetzungen

- **Docker Desktop** (20.10+)
- **Python** (3.10+)
- **Poetry** (1.5+)
- **Git**

> **âš ï¸ WICHTIG: Docker Desktop & WSL2 unter Windows**
>
> Das Framework nutzt Docker Desktop mit WSL2 fÃ¼r alle Build-Prozesse. Dies ist eine **zwingende Voraussetzung**.
>
> 1. Aktiviere **WSL2** (Windows Subsystem for Linux 2) Ã¼ber die PowerShell
> 2. Installiere das [WSL2 Linux-Kernel-Update-Paket](https://wslstore.blob.core.windows.net/wslupdate/wsl_update_x64.msi)
> 3. Installiere [Docker Desktop fÃ¼r Windows](https://docs.docker.com/desktop/install/windows-install/)
> 4. Stelle in den Docker-Einstellungen sicher, dass die **WSL2-Integration** aktiviert ist
>
> Das Framework prÃ¼ft automatisch, ob Docker lÃ¤uft, bevor die Installation fortgesetzt wird.

### Installation (Windows - Empfohlen)
1. Lade den neuesten [Installer (setup.exe)](Platzhalter-Link-zur-exe) herunter.
2. FÃ¼hre die Installation aus.
3. Starte "LLM-Builder" vom Desktop.

### Installation (Entwickler / Linux)
```bash
# 1. Repository klonen
git clone https://github.com/Smilez1985/llm_conversion_framework.git
cd llm_conversion_framework

# 2. Dependencies installieren (via Poetry)
poetry install

# 3. Docker-Container bauen (Initial)
docker-compose build

# 4. GUI starten
poetry run llm-builder
```

---

## ğŸ› ï¸ Verwendung

### Schritt 1: Hardware-Profil erstellen

FÃ¼hre dieses Skript auf deinem Zielsystem (z.B. dem Rockchip Board) aus, um die Hardware-FÃ¤higkeiten exakt zu erfassen.
```bash
# Auf deinem RK3566/Zielsystem ausfÃ¼hren
curl -O https://raw.githubusercontent.com/Smilez1985/llm_conversion_framework/main/scripts/hardware_probe.sh
chmod +x hardware_probe.sh
./hardware_probe.sh
# -> Erzeugt: target_hardware_config.txt
```

### Schritt 2: Modell konvertieren & bauen

**Via GUI** (empfohlen):

1. `File` â†’ `Import Hardware Profile` â†’ WÃ¤hle deine `target_hardware_config.txt`
2. WÃ¤hle im Tab **"Build & Monitor"** dein Modell (z.B. via `Browse HF` Button)
3. WÃ¤hle das Ziel (z.B. `rockchip`) und die Quantisierung (`Q4_K_M`)
4. Klicke `Start Build`

**Oder via CLI:**
```bash
poetry run llm-cli build \
  --model models/granite-h-350m \
  --target rockchip \
  --quantization Q4_K_M \
  --hardware-profile configs/my_rk3566.txt
```

### Schritt 3: Deployment

Das fertige Paket findest du im `output` Ordner.
```bash
cd output/packages/granite-h-350m_q4km_aarch64_latest/

# Kopiere diesen Ordner auf dein GerÃ¤t und fÃ¼hre aus:
./deploy.sh /opt/ai_models/
```

---

## ğŸ—ï¸ Architektur

### Framework-Struktur
```
llm-cross-compiler-framework/
â”œâ”€â”€ orchestrator/           # Python Core (GUI, CLI, Manager)
â”‚   â”œâ”€â”€ gui/                # GUI Fenster & Dialoge
â”‚   â”œâ”€â”€ Core/               # GeschÃ¤ftslogik
â”‚   â””â”€â”€ utils/              # Helper & Updater
â”œâ”€â”€ targets/                # Hardware-Module
â”‚   â”œâ”€â”€ rockchip/           # âœ… Production-Ready (RK3566/88)
â”‚   â”œâ”€â”€ _template/          # ğŸ“‹ Vorlage fÃ¼r neue Targets
â”‚   â””â”€â”€ ...
â”œâ”€â”€ community/              # Community-Contributed Targets
â”œâ”€â”€ configs/                # Globale Konfigurationen
â””â”€â”€ scripts/                # Setup, Build & CI Tools
```

### Pipeline-Ablauf
```
Input Model (HF/ONNX)
        â†“
    Format Convert
        â†“
    GGUF FP16
        â†“
Quantize (Native x86) â†â”€â”€â”€â”€ Hardware Profile
        â†“                           â†“
  Quantized GGUF            Config Module
        â†“                           â†“
        â””â”€â”€â”€â”€â”€â”€â†’ Cross-Compile â†â”€â”€â”€â”€â”˜
                       â†“
                llama-cli (ARM64)
                       â†“
              Deployment Package
```

### UnterstÃ¼tzte Hardware

| Familie | Status | Architekturen | Features |
|---------|--------|---------------|----------|
| **Rockchip** | âœ… Ready | RK3566, RK3568, RK3576, RK3588 | NEON, Cross-Compilation |
| **NVIDIA Jetson** | ğŸš§ Development | Nano, Xavier NX, Orin | CUDA, TensorRT |
| **Raspberry Pi** | ğŸš§ Development | Pi 4, Pi 5 | ARM Cortex-A72/A76 |
| **Intel NPU** | ğŸ“‹ Planned | Meteor Lake | OpenVINO |
| **Hailo** | ğŸ“‹ Planned | Hailo-8, Hailo-10 | HailoRT |

---

## ğŸ¤ Community & Beitragen

Wir brauchen **DICH**, um UnterstÃ¼tzung fÃ¼r weitere Hardware hinzuzufÃ¼gen!

### Neues Target hinzufÃ¼gen

Das Framework besitzt einen integrierten **5-Schritt Module Creation Wizard**:

1. Starte die GUI: `poetry run llm-builder`
2. MenÃ¼: `Tools` â†’ `Create New Module...`
3. Folge den **5 Schritten** (Hardware Info, Docker Setup, Flags, etc.)
4. Das Framework generiert automatisch alle notwendigen Skripte (`config_module.sh`, `Dockerfile`, etc.)

**Oder manuell:**
```bash
cp -r targets/_template targets/my_hardware
# targets/my_hardware/ anpassen
```

### Pull Requests

1. **Fork** das Repository
2. **Branch** erstellen: `git checkout -b feature/my-new-target`
3. **Module entwickeln** mit dem Wizard
4. **Tests** hinzufÃ¼gen und ausfÃ¼hren
5. **Pull Request** erstellen

### Community-Targets

Die `community/` Directory enthÃ¤lt von der Community beigesteuerte Hardware-Targets:

- `community/hailo/` - Hailo NPU Support
- `community/intel-npu/` - Intel Meteor Lake NPU
- `community/custom-boards/` - Spezial-Hardware

---

## ğŸ“Š Status & Roadmap

### Current Status (v1.1.0)

- âœ… **Framework Core** - GUI, CLI, Docker-Management
- âœ… **Rockchip Target** - Production-ready fÃ¼r RK3566/3588
- âœ… **Module Creation Wizard** - 5-Schritt Assistent
- âœ… **Auto-Update System** - Smart-Sync Technologie

### Roadmap

| Meilenstein | Status | Geplant |
|-------------|--------|---------|
| v1.0.0 (MVP) | âœ… | Rockchip RK3566/88 Support, GUI, Docker-Core |
| v1.1.0 | âœ… | Auto-Updater, Community Hub, Smart Sync |
| v1.2.0 | ğŸ“‹ | Intel NPU & Hailo Support |
| v2.0.0 | ğŸ“‹ | Cloud Build Integration & Auto-Optimization |

---

## ğŸ† Examples

### Rockchip RK3566 Example
```bash
# Hardware-Profil erstellen (auf RK3566)
./hardware_probe.sh

# Build via CLI
poetry run llm-cli build \
  --model models/granite-h-350m \
  --target rockchip \
  --quantization Q4_K_M \
  --hardware-profile configs/rk3566_profile.txt

# Output: granite-h-350m_q4km_aarch64.zip
# EnthÃ¤lt: Quantisiertes Model + AArch64 Binary + Test Scripts
```

### Performance Expectations

| Model | Hardware | Quantization | RAM Usage | Speed (tokens/s) |
|-------|----------|-------------|-----------|------------------|
| Granite-350M | RK3566 | Q4_K_M | ~200MB | 8-15 |
| Llama-2-7B | RK3588 | Q4_K_M | ~4GB | 5-10 |
| Phi-2-2.7B | Pi 5 | Q5_K_M | ~2GB | 3-8 |

---

## ğŸ“š Documentation

- ğŸ“– [Getting Started Guide](docs/getting-started.md)
- ğŸ”§ [Adding New Targets](docs/adding-targets.md)
- ğŸ“¡ [API Reference](docs/api-reference.md)
- ğŸ’¡ [Examples & Tutorials](docs/examples/)

---

## ğŸ› ï¸ Development

### Testing
```bash
# Framework-Tests
poetry run pytest

# Target-Validation
./scripts/validate-target.sh targets/rockchip

# Integration-Test
poetry run llm-cli test --target rockchip --model test-model
```

### Module-Entwicklung Guidelines

**Goldstandard-Direktiven fÃ¼r alle Module:**

**Docker-Container:**
- âœ… Multi-Stage Build verwenden
- âœ… BuildX fÃ¼r Multi-Architektur
- âœ… Hadolint-konforme Syntax
- âœ… Poetry fÃ¼r Python-Dependencies

**Scripts (Shell/Python):**
- âœ… VollstÃ¤ndig funktionsfÃ¤hig (keine Platzhalter)
- âœ… Robuste `if not exist` Abfragen
- âœ… Professional dokumentiert/kommentiert
- âœ… Isolierte Umgebungen (Container-native)

---

## ğŸ“„ Lizenz

Dieses Projekt ist lizenziert unter der **MIT License** - siehe die [LICENSE](LICENSE) Datei fÃ¼r Details.

---

## ğŸ™ Danksagung

- **[llama.cpp](https://github.com/ggerganov/llama.cpp)** - Das HerzstÃ¼ck der Inferenz
- **[Hugging Face](https://huggingface.co/)** - FÃ¼r das Modell-Ã–kosystem
- **[Radxa Community](https://forum.radxa.com/)** - FÃ¼r den Support bei der RK3566 Integration
- **[Docker](https://www.docker.com/)** - Containerization Platform
- **[PySide6](https://doc.qt.io/qtforpython-6/)** - Professional GUI Framework
- **[Poetry](https://python-poetry.org/)** - Modern Python Dependency Management

---

<div align="center">

**Built with â¤ï¸ for the Edge AI Community**

*Empowering developers to run AI everywhere.*

</div>
