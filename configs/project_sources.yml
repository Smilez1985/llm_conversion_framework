# Zentrale Quellen für das LLM Cross-Compiler Framework
# Status: Verifiziert & Community-Ready & Secured
#
# Diese Datei ist die "Single Source of Truth" (SSOT).
# Der Orchestrator lädt diese Links und injiziert sie als Umgebungsvariablen
# in die Docker-Container. Ditto (AI) nutzt die 'docs_workflow' Links.

# --- BUILD TOOLS (Security Pinned) ---
build_tools:
  poetry_installer: 
    url: "https://install.python-poetry.org"
    # SHA256 Hash des Installers (Verifiziert Nov 2025)
    sha256: "3b5842cd6318176812455429c6c109909d21b184579720f154506f6734828455" 

# --- CORE FRAMEWORK ---
core:
  # Core Inference Engine für CPU
  llama_cpp: 
    url: "https://github.com/ggerganov/llama.cpp.git"
    commit: "b3626" # Pinned Commit für Reproduzierbarkeit
   
  # Python Transformers für Konvertierungs-Skripte
  transformers: "https://github.com/huggingface/transformers"

# --- VECTOR DATABASE (Local RAG / Knowledge Base) ---
# NEU in v1.5.0: Qdrant für lokale Dokumenten-Indizierung
vector_database:
  qdrant_core:
    url: "https://github.com/qdrant/qdrant"
    # Pinned Tag v1.16.0 (Stable Release) für Reproduzierbarkeit
    commit: "v1.16.0" 
    # WICHTIG: Dies ist die Quelle für Ditto, um zu lernen, wie man Qdrant nutzt
    docs_workflow: "https://qdrant.tech/documentation/quickstart/"

# --- ROCKCHIP (RK3566 / RK3588) ---
rockchip_npu:
  # LLM-spezifische NPU Inferenz (nur RK3588/RK3576)
  rkllm_toolkit:
    url: "https://github.com/airockchip/rknn-llm.git"
    # Fallback-Version, die auch im Dockerfile verbacken ist
    commit: "release-v1.2.1b1" 
    docs_workflow: "https://github.com/airockchip/rknn-llm/blob/main/README.md"

  # Generelles NPU Toolkit (Vision/Audio - RK3566/RK3588)
  rknn_toolkit2: 
    url: "https://github.com/airockchip/rknn-toolkit2.git"
    # Fallback-Version, passend zum Dockerfile
    commit: "v2.0.0-beta0" 
    docs_workflow: "https://github.com/airockchip/rknn-toolkit2/tree/master/doc"
  
  # Model Zoo
  rknn_model_zoo: "https://github.com/airockchip/rknn_model_zoo"

# --- NVIDIA JETSON (Orin / Xavier / Nano) ---
nvidia_jetson:
  tensorrt_llm: 
    url: "https://github.com/NVIDIA/TensorRT-LLM.git"
    commit: "v0.9.0" 
  jetson_inference: "https://github.com/dusty-nv/jetson-inference"
  jetson_containers: "https://github.com/dusty-nv/jetson-containers"
  docs_workflow: "https://nvidia.github.io/TensorRT-LLM/architecture/workflow.html"

# --- HAILO AI (Raspberry Pi 5 + Hailo-8 / 8L) ---
hailo_ai:
  hailort: 
    url: "https://github.com/hailo-ai/hailort.git"
    commit: "v4.17.0"
  tappas: "https://github.com/hailo-ai/tappas"
  hailo_rpi5_examples: "https://github.com/hailo-ai/hailo-rpi5-examples"
  docs_workflow: "https://github.com/hailo-ai/hailo_model_zoo/blob/master/docs/public_models/HAILO8/HAILO8_inference_guide.rst"

# --- INTEL NPU (Core Ultra / Meteor Lake) ---
intel_npu:
  openvino: "https://github.com/openvinotoolkit/openvino"
  linux_npu_driver: "https://github.com/intel/linux-npu-driver"
  docs_workflow: "https://docs.openvino.ai/2024/documentation/openvino-ir-format.html"

# --- AMD ROCm (Radeon GPUs) ---
amd_rocm:
  rocm_main: "https://github.com/ROCm/ROCm"
  rocm_docker: "https://github.com/ROCm/ROCm-docker"

# --- RISC-V (StarFive / Generic) ---
riscv:
  gnu_toolchain: "https://github.com/riscv-collab/riscv-gnu-toolchain"
  visionfive2_sdk: "https://github.com/starfive-tech/VisionFive2"

# --- VOICE & TTS PIPELINE ---
voice_tts:
  piper_tts: 
    url: "https://github.com/rhasspy/piper"
    commit: "9c3066d" 
  glados_tts: "https://github.com/dnhkng/GLaDOS"
  vosk_api: 
    url: "https://github.com/alphacep/vosk-api"
    commit: "0a1b2c3" 

# --- MODELLE (MVP / Getestet) ---
models:
  granite_350m: "https://huggingface.co/ibm-granite/granite-4.0-h-350m"
