# Zentrale Quellen für das LLM Cross-Compiler Framework
# Status: Verifiziert & Community-Ready & Secured
#
# Diese Datei ist die "Single Source of Truth" (SSOT).
# Der Orchestrator lädt diese Links und injiziert sie als Umgebungsvariablen
# in die Docker-Container. Ditto (AI) nutzt die 'docs_workflow' Links.

# --- BUILD TOOLS (Security Pinned) ---
build_tools:
  poetry_installer: 
    url: "https://install.python-poetry.org"
    # SHA256 Hash des Installers (Verifiziert Nov 2025)
    sha256: "3b5842cd6318176812455429c6c109909d21b184579720f154506f6734828455" 

# --- CORE FRAMEWORK ---
core:
  llama_cpp: 
    url: "https://github.com/ggerganov/llama.cpp.git"
    commit: "b3626" 
  transformers: "https://github.com/huggingface/transformers"

# --- VECTOR DATABASE (Local RAG / Knowledge Base) ---
# NEU v2.0: Vollständige Quellen für Qdrant (Docker, SDK, Docs)
vector_database:
  qdrant_core:
    repo_url: "https://github.com/qdrant/qdrant"
    docker_image: "qdrant/qdrant"
    [cite_start]docker_tag: "v1.12.0" # Pinned Stable [cite: 57]
    # Offizielle Doku für Ditto (Deep Ingest Ziel)
    docs_workflow: "https://qdrant.tech/documentation/"
    docs_snapshots: "https://qdrant.tech/documentation/concepts/snapshots/"
  
  qdrant_client:
    repo_url: "https://github.com/qdrant/qdrant-client"
    # Python SDK Doku
    docs_sdk: "https://python-client.qdrant.tech/"

# --- KNOWLEDGE INGESTION (v1.6.0) ---
knowledge_ingestion:
  langchain:
    url: "https://github.com/langchain-ai/langchain"
    docs_workflow: "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/"
  beautifulsoup4:
    url: "https://git.launchpad.net/beautifulsoup"
    docs_workflow: "https://www.crummy.com/software/BeautifulSoup/bs4/doc/"
  pypdf:
    url: "https://github.com/py-pdf/pypdf"
    docs_workflow: "https://pypdf.readthedocs.io/en/stable/"

# --- INTEL NPU & GPU (v1.7.0) ---
intel_ecosystem:
  ipex_llm:
    url: "https://github.com/intel/ipex-llm"
    commit: "v2.2.0" 
    docs_workflow: "https://github.com/intel/ipex-llm/blob/main/docs/mddocs/Quickstart/install_linux.md"
  openvino:
    url: "https://github.com/openvinotoolkit/openvino"
    commit: "2025.4.0" 
    docs_workflow: "https://docs.openvino.ai/2025/documentation/openvino-ir-format.html"

# --- ROCKCHIP (RK3566 / RK3588) ---
rockchip_npu:
  rkllm_toolkit:
    url: "https://github.com/airockchip/rknn-llm.git"
    commit: "release-v1.2.1b1" 
    docs_workflow: "https://github.com/airockchip/rknn-llm/blob/main/README.md"
  rknn_toolkit2: 
    url: "https://github.com/airockchip/rknn-toolkit2.git"
    commit: "v2.0.0-beta0" 
    docs_workflow: "https://github.com/airockchip/rknn-toolkit2/tree/master/doc"
  rknn_model_zoo: "https://github.com/airockchip/rknn_model_zoo"

# --- NVIDIA JETSON ---
nvidia_jetson:
  tensorrt_llm: 
    url: "https://github.com/NVIDIA/TensorRT-LLM.git"
    commit: "v0.9.0" 
  jetson_inference: "https://github.com/dusty-nv/jetson-inference"
  jetson_containers: "https://github.com/dusty-nv/jetson-containers"
  docs_workflow: "https://nvidia.github.io/TensorRT-LLM/architecture/workflow.html"

# --- HAILO AI ---
hailo_ai:
  hailort: 
    url: "https://github.com/hailo-ai/hailort.git"
    commit: "v4.17.0"
  tappas: "https://github.com/hailo-ai/tappas"
  hailo_rpi5_examples: "https://github.com/hailo-ai/hailo-rpi5-examples"
  docs_workflow: "https://github.com/hailo-ai/hailo_model_zoo/blob/master/docs/public_models/HAILO8/HAILO8_inference_guide.rst"

# --- OTHER TARGETS ---
amd_rocm:
  rocm_main: "https://github.com/ROCm/ROCm"
  rocm_docker: "https://github.com/ROCm/ROCm-docker"
riscv:
  gnu_toolchain: "https://github.com/riscv-collab/riscv-gnu-toolchain"
  visionfive2_sdk: "https://github.com/starfive-tech/VisionFive2"

# --- VOICE & TTS ---
voice_tts:
  piper_tts: 
    url: "https://github.com/rhasspy/piper"
    commit: "9c3066d" 
  glados_tts: "https://github.com/dnhkng/GLaDOS"
  vosk_api: 
    url: "https://github.com/alphacep/vosk-api"
    commit: "0a1b2c3" 

# --- MODELLE ---
models:
  granite_350m: "https://huggingface.co/ibm-granite/granite-4.0-h-350m"

# --- TINY MODELS (Offline Intelligence) ---
tiny_models:
  tinyllama_1b:
    url: "https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0"
    description: "Fastest option (< 700MB)."
  qwen_0_5b:
    url: "https://huggingface.co/Qwen/Qwen1.5-0.5B-Chat"
    description: "Extremely smart for size (< 500MB)."
  danube_1_8b:
    url: "https://huggingface.co/h2oai/h2o-danube-1.8b-chat"
    description: "Strong reasoning capabilities."
